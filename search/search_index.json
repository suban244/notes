{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Obsidian Notes","text":"<p>Hello there, we got some notes here. Get started with ViT</p>"},{"location":"#topics","title":"Topics","text":"<p>Learning Resources of Research Interns  Architectures </p> <pre><code>CNNs\n\nRNNs (mostly LSTMs)\n\nWhy is it almost replaced by transformers?\n\nWhich cases do RNNs work better?\n\nTransformers\n\nEncoder-only variants (mainly used)\n\nSome recent advancements in decoder-only models or encoder-decoder models\n\nViT and its variants like Swin (Still transformers, but transferred to CV from NLP)\n</code></pre> <p>Learning Paradigms </p> <pre><code>Supervised\n\nSemi-supervised\n\nSelf-supervised\n\nUnsupervised\n\nDifferences and use cases for each\n</code></pre> <p>We mainly work on supervised learning. But some work ahead needs the knowledge of semi-supervised and self-supervised learning. </p> <p>Domains </p> <p>NLP  - Language Modelling - Next Word Prediction vs Masked Token Prediction</p> <ul> <li>NER </li> <li>Subword tokenization challenges: how to combine the predictions? </li> <li>POS Tagging </li> <li>Sentiment Analysis (esp. Target-aspect-based sentiment analysis; TABSA; some effort needed in this) </li> </ul> <p>Computer Vision (esp. Medical Imaging)   - Segmentation  - ViT  - SWIN</p> <ul> <li>Object Detection </li> <li>Use pretrained models on each of the above tasks </li> </ul> <p>Multimodal Learning (Vision and Language; VLM; VLSM)  - CLIP and its variants (OpenCLIP, MetaCLIP) (VLM)  - CRIS</p> <pre><code>VLSMs\n\nCLIPSeg\n\nZegCLIP\n\nInclude others\n</code></pre>"},{"location":"Datasets/","title":"Datasets","text":"<p>About bele bele - Ek rat basera format milaye milxa, and will be a good evaluation question</p>"},{"location":"Donut%20Peering/","title":"Internet Infrastructure","text":"<p>Initially internet was designed with only a few nodes and with packets that could be taken apart and put together in sequence. Also there was open connection between nodes for packet transmission. With the increase in usage of internet, it has evolved to millions of nodes and the packet needs to travel through multiple networks before it gets delivered. The packet then may get dropped, lost, or get rerouted to a different route making putting together packets difficult. </p>"},{"location":"Donut%20Peering/#tier-1-networks","title":"Tier 1 Networks","text":"<p>These are the original networks. They have settlement-free peering with one another. They have massive routing tables and can send traffic throughout the internet. - Since peering is free of cost, the is cost effective to send traffic through Tier other 1 networks</p>"},{"location":"Donut%20Peering/#tier-2-networks","title":"Tier 2 networks","text":"<p>A network that primarily peers with other networks but also purchases IP transit to a portion of the internet. These networks are the most common providers are the closest to the 'edge', end users, for example: cable providers and regional ISPs. </p>"},{"location":"Donut%20Peering/#video-streaming-data","title":"Video Streaming Data","text":"<p>Video streaming will require a constant stream of data.The more networks video data has to travel between networks, the less quality the video will be, mainly because of packet loss and delays. </p>"},{"location":"Donut%20Peering/#traditional-tier-1-routing","title":"Traditional Tier 1 Routing","text":"<p>Traditionally the internet relied on inter-network infrastructure and peering between Tier 1 networks, which acted as backbone providers. Tier 2 networks payed Tier 1 networks to access their one networks. For example an ISP user, say of Broadband Provider, wants to receive data hosted by Content Provider. The data query is  1) Transmitted upstream by Broadband provider 2) Transmitted to a Tier 1 provider, from whom Broadband provider is taking service 3) Transmitted to a Tier 2 provider, from whom Content Provider is taking service 4) To the content provider network </p>"},{"location":"Donut%20Peering/#case-for-direct-peering","title":"Case for direct peering","text":"<p>Tier 2 networks pay to sending traffic through the Tier 1 network, which is also slow, and more prone to packet loss. Conversely, it would make more sense to directly peer with other Tier 2 networks to exchange traffic. The cost stays, but each packet would need less hops and there will be less packet loss, thus better service to the end users. </p> <p>Mzima Networks, leverages this to provide streaming content more efficiently. It peers with both  - Tier 2 networks:  for faster, cost effective and less error prone delivery - Tier 1 network: To make sure there are no holes in the routing table This helps create a donut shaped network shown here.  This structure provides - less packet loss and latency for end users - lower costs - resiliency through multiple connections.</p>"},{"location":"Donut%20Peering/#case-study-of-babulous-a-streaming-content-provider","title":"Case Study of Babulous, a Streaming Content Provider","text":"<p>It used this technology provides by Mzima Networks to stream content to its users, at high speeds, ensure that there are no pauses or other issues during music streaming.</p>"},{"location":"NLP/BERT/","title":"BERT","text":""},{"location":"NLP/BERT/#embeddings","title":"Embeddings","text":"<ul> <li>Word2Vec</li> </ul>"},{"location":"NLP/BERT/#elmo-context-matters","title":"ELMo: Context Matters","text":"<ul> <li>words have different meaning depending on the context</li> <li>\"stick\": \"let's stick to the plan\" vs \"i used a walking stick\"</li> <li>Use of bidirectional LSTM to look at the whole context</li> <li>Trained on predicting the next word  </li> </ul>"},{"location":"NLP/BERT/#ulm-fit","title":"ULM-FiT","text":"<ul> <li>Utilize a lot of what model learns during pretraining</li> <li>Introduces a way to do transfer learning</li> </ul>"},{"location":"NLP/BERT/#openai-transformer","title":"OpenAI Transformer","text":"<p>Decoder model for language modeling - Stack 12 decoders and throw 7000 books at them     - books good, long context - Trained using next word prediction, forward only language model     - Issue: Only context from one side     - We need to mask the next token as to not let embedding leak in </p>"},{"location":"NLP/BERT/#bert","title":"BERT","text":"<p>Encoder model for language modeling </p>"},{"location":"NLP/BERT/#masked-language-model","title":"Masked Language model","text":""},{"location":"NLP/BERT/#two-sentence-task","title":"Two sentence Task","text":"<p>From this task, it is assumed that the bert model learns to encapsulate the entire information of a sentence in the <code>[CLS]</code> token </p>"},{"location":"NLP/BERT/#bert-on-different-tasks","title":"BERT On different Tasks","text":""},{"location":"NLP/BERT/#bert-as-a-feature-extractor","title":"BERT as a feature extractor","text":""},{"location":"NLP/Language%20Modelling/","title":"Language Modelling","text":""},{"location":"NLP/Language%20Modelling/#input-representation","title":"Input Representation","text":"<ul> <li>Positional Embeddings</li> <li>Segment Embeddings<ul> <li><code>Sentence A</code> &amp; <code>Sentence B</code></li> </ul> </li> <li>Token Embeddings <ul> <li>WordPiece splitting (Subword Tokenization)</li> <li>BARD: 30K  wordpiece embeddings</li> </ul> </li> </ul>"},{"location":"NLP/Language%20Modelling/#hackers-guide-to-language-models","title":"Hacker's Guide To Language Models","text":"<p>https://www.youtube.com/watch?v=jkrNMKz9pWU - Language modeling -&gt; Compression - LM pre-training -&gt; LM fine-tuning -&gt; Classifier Fine-tuning - When it comes to fine tuning, use of dataset like OpenOrca:      - \"Question: ...? Answer: \"     -  Then the model does the next word prediction     - Classifier Fine-Tuning: RLHF and friends</p>"},{"location":"NLP/Language%20Modelling/#fine-tuning-procedure","title":"Fine Tuning Procedure","text":""},{"location":"NLP/Language%20Modelling/#large-language-models","title":"Large Language Models","text":"<ul> <li>Obtaining a base model</li> <li>Compressing the information into a file. (lossy compression)<ul> <li>For Llama 2 70B: <ul> <li>~10TB  </li> <li>6,000 GPUs for 12 Days ($2M)</li> <li>~140GB File</li> </ul> </li> <li>For things like bard, 10x more computationally complex</li> </ul> </li> <li>Trained Using [[Next Word Prediction]] </li> <li>Can combine information from multiple sources.</li> </ul>"},{"location":"NLP/Language%20Modelling/#assistant-models","title":"Assistant Models","text":"<ul> <li>Query/Answer kind of models</li> <li>Optimization task stays the same, Swap out the dataset</li> <li>Hire people to make the data: ~100K conversations.</li> <li>Quality &gt; Quantity</li> <li>Model able to access the knowledge of the pre-training phase, while keeping the structure of fine-tuning stage (alignment) </li> </ul>"},{"location":"NLP/Language%20Modelling/#comparasion-label","title":"Comparasion Label","text":"<ul> <li>The third stage, Reinforcement Learning with Human Feedback (RLHF)</li> <li>Choosing which output of language model is better</li> </ul>"},{"location":"NLP/Language%20Modelling/#bert","title":"BERT","text":""},{"location":"NLP/Language%20Modelling/#some-language-models","title":"Some language models","text":""},{"location":"NLP/Language%20Modelling/#roberta","title":"Roberta","text":"<ul> <li>More data good</li> <li>More compute good</li> </ul>"},{"location":"NLP/Language%20Modelling/#xlnet","title":"XLNET","text":"<ul> <li>Relative positional embedding<ul> <li>How much should <code>dog</code> attend to previous word?</li> <li>Generalizes better to long sequences</li> </ul> </li> <li>Permutation Language Model<ul> <li>Instead of left to write, permute the sentence and do a sort of next word prediction on that</li> <li>Equavalent to masking, but more predictions per sentencePermutation Language Modeell</li> <li>Instead of left to write, permute the permute the sentence and do a sort of next word prediction on that</li> <li>Equivalent to masking, but more predictions per sentence</li> </ul> </li> </ul>"},{"location":"NLP/Language%20Modelling/#albert","title":"ALBERT","text":"<ul> <li>Parameter sharing<ul> <li></li> <li>Less over-fitting when fine-tuning</li> </ul> </li> <li>Cross Layer parameter sharing<ul> <li>Share all parameters between transformer layers</li> </ul> </li> <li>ALBERT light in terms of parameters, but not speed</li> </ul>"},{"location":"NLP/Language%20Modelling/#t5","title":"T5","text":"<ul> <li>Exploring limits of transformers</li> <li>big model 11B</li> <li>All that matters is making model bigger, and more clean data</li> </ul>"},{"location":"NLP/Language%20Modelling/#electra","title":"Electra","text":""},{"location":"NLP/Language%20Modelling/#distillation","title":"Distillation","text":"<ul> <li>Model Compression</li> <li>Train <code>Teacher</code>: use the best techniques to get the best model ( pre-training + fine tuning)</li> <li>Label a large amount of data using the <code>Teacher</code></li> <li>Train <code>Student</code>: Train a smaller model (x50 smaller) to mimic Teacher output</li> <li>Mean Square / Cross Entropy</li> </ul>"},{"location":"NLP/Language%20Modelling/#why-does-this-work","title":"Why does this work?","text":"<ul> <li>Language models are like the Ultimate Model, and any other fine tuned model will only use a subset of the abilities of the LM</li> <li>Thus the distilled model only has the features relevant to the task</li> <li>How does distilation work with search??</li> </ul> <p>bard to token replacement??</p>"},{"location":"NLP/Named%20Entity%20Recognition/","title":"Named Entity Recognition","text":"<p>Finding names of people, organizations, date, etc in unorganized text - One approach can be to make a massive table - But the same word can be used in multiple contexts     - Mars: location, food item - Process word by word, assigning a label to each </p>"},{"location":"NLP/Named%20Entity%20Recognition/#feed-forward-ner","title":"Feed Forward NER","text":"<p> Works suprizingly well, just need embeddings and some data. But we can do better. </p>"},{"location":"NLP/Next%20Word%20Prediction%20vs%20Masked%20Token%20Prediction/","title":"Next Sentence Prediction","text":"<ul> <li>Say you are trying to predict using next word prediction in a wiki article about a person X. </li> <li>Now you will need to know about when person born, what they did, and so on.</li> <li>For learning relationship between sentences, using sentences from same document and different documents</li> </ul>"},{"location":"NLP/Next%20Word%20Prediction%20vs%20Masked%20Token%20Prediction/#masked-language-model","title":"Masked Language Model","text":"<ul> <li>Mask <code>k%</code> of input words <code>This is a [MASK] sentence. This is a bad [MASK].</code></li> <li>Issue<ul> <li>Low Number of predictions per sentence</li> <li>Too little masking: too expensive to train</li> <li>Too much masking: not enough context</li> </ul> </li> </ul>"},{"location":"NLP/Next%20Word%20Prediction%20vs%20Masked%20Token%20Prediction/#masked-token-prediction","title":"Masked Token Prediction:","text":"<ul> <li>Lower number of predictions compared to next word prediction</li> </ul>"},{"location":"NLP/Next%20Word%20Prediction%20vs%20Masked%20Token%20Prediction/#next-word-prediction","title":"Next word Prediction","text":"<ul> <li>Only uses left context(or right context) but Language understanding is bidirectional </li> <li>Issue: words can \"see themselves\" in a bidirectional context  </li> <li>Masked LM takes longer to converge but does much better pretty fast</li> <li>At the very beginning, left-to-right does better, probably because more data</li> </ul>"},{"location":"NLP/Project/","title":"Project","text":"<ul> <li>Foundation Model: MuRIL (- tokerizer aru herne )     TODO: look for finance pre-trained models</li> <li>SQuAD ma fine tune (combining multiple datasets ?)</li> </ul>"},{"location":"NLP/Project/#performance-evaluation","title":"Performance Evaluation","text":"<p>Benchmarking dataset: - XQuAD (hindi) - gold standard dataset banaune ? - textual similarity  - every domain bata few datasets banaune ~300 examples for benchmark</p>"},{"location":"NLP/Project/#todo","title":"TODO","text":"<ul> <li>Translation garidai xa </li> <li>Fix domain(national economics) , collect data<ul> <li>public financial dataset -&gt; SQuAD </li> </ul> </li> <li>RAG: vector store redis<ul> <li>langchain</li> </ul> </li> <li>Build an application</li> </ul> <p>RAG: QA model -&gt; </p>"},{"location":"NLP/Resources/","title":"Resources","text":"<ul> <li>Stanford 224N https://www.youtube.com/watch?v=rmVRLeJRkl4&amp;list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4</li> <li>cs25 andrej karpathy https://www.youtube.com/watch?v=XfpMkf4rD6E</li> <li>Mask token article https://h2o.ai/wiki/mask-token/</li> <li>Pretraining language model https://www.youtube.com/watch?v=DGfCRXuNA2w</li> <li>Video https://www.youtube.com/watch?v=thr4-hgLhi8</li> </ul>"},{"location":"NLP/Subword%20Tokenization/","title":"Motivation","text":"<ul> <li>Language keeps evolving continuously</li> <li>Mapping all words to OOV is bad</li> <li>Mapping different vectors of different versions of words can be wasteful<ul> <li>eat, eating, eated, ..</li> </ul> </li> <li>Split words into sequences of known subwords. a woman eats rice t1 t2 t3 t4 t5 t6 a          wo ##man eat ##s rice (0, 1)</li> </ul>"},{"location":"NLP/Subword%20Tokenization/#description","title":"Description","text":"<ul> <li>Model treats sub word as a token, split words not treated differently.</li> <li>Very long words<ul> <li>If the long word is not very common, tough luck baby, natural selection (or stats)</li> </ul> </li> <li>Also, we generally try to split words into the lowest amount of subwords</li> <li>One could image with the word <code>watch</code> not being that common and <code>a</code> being super common, and the spitting <code>w##</code> <code>a##</code> <code>tch</code> is very bad</li> <li>Read this for nepali: https://aclanthology.org/2022.sigul-1.14.pdf</li> <li>Sentence Piece Embeddings: https://aclanthology.org/D18-2012.pdf#:~:text=SentencePiece%20implements%20the%20Decoder%20as%20an%20inverse%20operation,normalized%20text%20is%20preserved%20in%20the%20encoder%E2%80%99s%20output.</li> </ul>"},{"location":"NLP/Subword%20Tokenization/#byte-pair-encoding","title":"Byte-Pair Encoding","text":"<ul> <li>Start with characters and [EOS] token.</li> <li>Find common adjacent characters, combine it as a subword.</li> <li>Repeat until desired vocab size achieved.</li> </ul>"},{"location":"NLP/Subword%20Tokenization/#wordpiece","title":"WordPiece","text":"<ul> <li>Similarly to byte-pair</li> <li>Adds those pairs which maximize the likelihood of the training data</li> <li>i think this means Chooses the word that maximizes the equation, \\(l_1\\) and \\(l_2\\) are tokens  \\(\\(\\frac {P(l_1l_2)} {P(l_2 | l_1)}\\)\\) </li> <li>used in BERT</li> </ul>"},{"location":"NLP/Subword%20Tokenization/#unigram","title":"Unigram","text":"<ul> <li>Starts with a large vocab and progressively trims down</li> <li>Removes ~10% of tokens that would least decrease loss, each iteration.</li> <li>stores probablities of each token, and tokenizes to maximize the probablity.</li> <li>Used in conjunction with sentence piece</li> </ul>"},{"location":"NLP/Subword%20Tokenization/#sentencepiece","title":"SentencePiece","text":"<ul> <li>Takes the entire stream as input, including spaces, and uses BPE or unigram to construct vocab.</li> <li>Eg: albert, T5, ..</li> </ul>"},{"location":"NLP/Subword%20Tokenization/#random-examples","title":"Random Examples","text":"<ul> <li><code>---------------</code>: Tables idk</li> <li> <p><code>Tastyyyyyyyyyy</code>, <code>talbe</code> (misspelling)</p> </li> <li> <p>kina mbert le ramro gardaina </p> </li> <li> <p>Muril and mbert</p> </li> <li> <p>Bayes rule for finding which is good, </p> </li> </ul>"},{"location":"Techniques/Distilation/","title":"Distilation","text":"<p>Generally  - A large teacher model and a smaller student model. - We try to match the output logits of teacher model by the student model. </p>"},{"location":"Techniques/Distilation/#issue-with-softmax","title":"Issue with softmax","text":"<p>From the original paper - Softmax pushes non target labels towards very small values  - Softmax tends to hide the similarity between other classes (which are not the target)     - Example: Say a 2 can be more similar to a 7, than a 8. Insight: If we make the values of logits smaller before passing into softmax function, they retain some relativeness</p>"},{"location":"Techniques/Distilation/#temperature","title":"Temperature","text":"<p>Used to more smooth the output of softmax \\(\\(\\frac {\\exp(5/T)} {\\exp(5/T) + \\exp(1/T)}\\)\\) More generally. Here T is a hyper parameter \\(\\(\\frac {\\exp(z_i/T)} {\\sum_j \\exp(z_j/T)}\\)\\) We choose a T such that it removes the impurities but preserves the elements </p>"},{"location":"Techniques/Distilation/#losses","title":"Losses","text":"<p>Use of Cross Entropy Loss / L2 loss between the probablities</p>"},{"location":"Techniques/Distilation/#intermediate-match-weightsfeatures","title":"Intermediate match (Weights/Features)","text":"<ul> <li>FC used to align dimensions </li> </ul>"},{"location":"Techniques/Distilation/#ways-of-distillation","title":"Ways of distillation","text":""},{"location":"Techniques/Distilation/#function-matching","title":"Function Matching","text":"<p>Out of distribution data for teacher and the student</p>"},{"location":"Techniques/Label%20Smoothing/","title":"Label Smoothing","text":"<ul> <li>A type of regularization</li> <li>Change hard labels, one hot to smoother labels \\(\\epsilon\\) is small. \\(\\(\\large [0, 0, 1, 0] \\rightarrow [\\epsilon / 3, \\epsilon / 3, 1 - \\epsilon, \\epsilon /3 ]\\)\\)</li> </ul> <p>Because of softmax, we cannot really get an output that is one-hot encoded.  But rather we push the model into learning (before activation) final layer output of the form \\([-\\infty, -\\infty, \\infty, -\\infty]\\) - For this, weights become very large and this can cause model to over-fit - We might need to change the loss function for this</p>"},{"location":"Techniques/Transformer%20Embeddings/","title":"Relative-Positioning","text":"<ul> <li>For fields where putting position doesn't make sense. Eg: Graphs</li> <li>Each token doesn't have one positional embedding, but rather \\(N\\) positional embedding, for each token \\(N\\).   Now adding the positional information to the token can be an issue, and each token as \\(N\\) positional embedding. So we add this positional embedding information via the self-attention mechanism. In the original paper, this information is added to the Values and the keys. The exact values are learned.</li> <li>Also we may decide to clip the embedding at \\(k\\) distance</li> </ul>"},{"location":"Vision/CLIP/","title":"CLIP","text":"<ul> <li>Vision and Language representation</li> <li>Zero shot classifier<ul> <li>Uses natural language as a flexible prediction space</li> </ul> </li> <li>Two encoders: one for text and another for image(Resnet or ViT)<ul> <li>contrastive learning</li> </ul> </li> <li>Doesn't directly optimize for the benchmark</li> </ul>","tags":["zero-shot","natural-language-supervision","multimodal-learning","Vision-Transformers","OpenAI"]},{"location":"Vision/CLIP/#ingredients","title":"Ingredients","text":"<ul> <li>Data (400M text and image pairs)</li> <li>Contrastive pre-training</li> <li>Computational efficiency: transformers parallelism </li> <li>Visual &amp; Language Representation</li> </ul>","tags":["zero-shot","natural-language-supervision","multimodal-learning","Vision-Transformers","OpenAI"]},{"location":"Vision/CLIP/#why","title":"Why ?","text":"<ul> <li>typical vision data creation is very labor intensive.</li> </ul>","tags":["zero-shot","natural-language-supervision","multimodal-learning","Vision-Transformers","OpenAI"]},{"location":"Vision/CLIP/#issues","title":"issues","text":"<ul> <li>Scaling with higher compute</li> <li>abstract tasks like counting</li> <li>fine grained classification<ul> <li>Predicting the model of a car, species of a flower, etc</li> </ul> </li> <li>Data not in the pre-training dataset<ul> <li>like MNIST: hand written digits</li> </ul> </li> <li>some prompt engineering maybe required</li> <li>NOT Data efficient, but rather provides a method that can be scaled to supervise with millions of images.</li> </ul>","tags":["zero-shot","natural-language-supervision","multimodal-learning","Vision-Transformers","OpenAI"]},{"location":"Vision/CLIP/#zero-shot","title":"Zero-Shot","text":"<ul> <li>Previously approaches to make it work in the embedding space.<ul> <li>De-VISE</li> <li>FAIR</li> </ul> </li> </ul>","tags":["zero-shot","natural-language-supervision","multimodal-learning","Vision-Transformers","OpenAI"]},{"location":"Vision/CLIP/#applications","title":"Applications","text":"<ul> <li>An image search engine</li> <li>Discriminator for GANs</li> </ul>","tags":["zero-shot","natural-language-supervision","multimodal-learning","Vision-Transformers","OpenAI"]},{"location":"Vision/CRIS/","title":"CRIS","text":"<p>CLIP-Driven Referring Image Segmentation</p>","tags":["VLM","VLSM","natural-language-supervision","multimodal-learning","Vision-Transformers","zero-shot"]},{"location":"Vision/CRIS/#overview","title":"Overview","text":"<p>Vision Language Decoder - Transfer fine grained semantic information from text to pixel level features(patches) - Self-Attention: for long range dependencies - Cross-Attention: for fine structured textual features to pixel level features Text To Pixel contrastive Learning - Explicitly enforce the textual similarity - Align language features with corresponding pixel features</p>","tags":["VLM","VLSM","natural-language-supervision","multimodal-learning","Vision-Transformers","zero-shot"]},{"location":"Vision/CRIS/#architecture","title":"Architecture","text":"<p>![[Pasted image 20240106103205.png]]</p>","tags":["VLM","VLSM","natural-language-supervision","multimodal-learning","Vision-Transformers","zero-shot"]},{"location":"Vision/CRIS/#feature-extraction","title":"Feature-Extraction","text":"<ul> <li>Image Encoder: Res-net<ul> <li>Use of 2nd-4th stage features</li> </ul> </li> <li>Text Encoder: Standard encoder<ul> <li>\\(F_s\\): <code>[CLS]</code> token</li> </ul> </li> <li>Cross Modal Neck: incorporate the textual embedding into image features</li> <li>Use of [[CoordConv]]</li> </ul>","tags":["VLM","VLSM","natural-language-supervision","multimodal-learning","Vision-Transformers","zero-shot"]},{"location":"Vision/CRIS/#vision-language-decoder","title":"Vision-Language-Decoder","text":"<p>Takes input \\(F_v \\in R^{N \\times C}\\) and \\(F_t \\in R^{L \\times C}\\) generates multi-model features \\(F_c \\in R^{N \\times C}\\). To capture positional information, sine spacial positional embeddings are added. - Multi head self attention \\(\\(F^\u2032_v = M HSA(LN (F_v)) + F_v\\)\\) - Multi head cross attention $$ F^\u2032_c = M HCA(LN (F^\u2032_v), F_t) + F^\u2032_v$$ - MLP block $$ F_c = MLP (LN (F^\u2032_c)) + F ^\u2032_c$$</p>","tags":["VLM","VLSM","natural-language-supervision","multimodal-learning","Vision-Transformers","zero-shot"]},{"location":"Vision/CRIS/#contrastive-learning","title":"Contrastive-Learning","text":"<p>We get a \\(F_c \\in R^{N \\times C}\\) from vision language decoder and \\(F_s \\in R^{C^{'}}\\) from text feature extractor.  We transform this by $$\\large z_v = F^\u2032_ cW_v + b_v, F^\u2032_c = U p(F_c) $$\\(\\(\\large z_t = F_sW_t + b_t,\\)\\) - \\(z_t \u2208 R^D\\), \\(z_v \u2208 R^{N\u00d7D}\\) , \\(N = H/4 \u00d7W/4\\)  - Up denotes 4\u00d7 up-sampling - Then it is trained constrastive style </p>","tags":["VLM","VLSM","natural-language-supervision","multimodal-learning","Vision-Transformers","zero-shot"]},{"location":"Vision/DETR/","title":"Architecture Overview","text":""},{"location":"Vision/DETR/#bipartite-loss","title":"Bipartite Loss","text":""},{"location":"Vision/DeIT/","title":"DeIT","text":"<p>Data efficient image transformers &amp; distillation through attention - Heavy image augmentation - Distillation token - Regularization</p>","tags":["Vision-Transformers","Knowledge-Distillation","facebook"]},{"location":"Vision/DeIT/#results","title":"Results","text":"<ul> <li>competitive result at imagenet with no external data and less compute<ul> <li>When pretrained on imagenet, good result on downstream tasks.</li> </ul> </li> </ul>","tags":["Vision-Transformers","Knowledge-Distillation","facebook"]},{"location":"Vision/DeIT/#regularization","title":"Regularization","text":"<ul> <li>[[Erasing]]</li> <li>[[Stochastic Depth]]</li> <li>[[Repeated Augmentation]] The first two being crucial for convergence Not applied but cool</li> <li>[[Dropout]]</li> <li>[[Exponentially Moving average]]</li> </ul>","tags":["Vision-Transformers","Knowledge-Distillation","facebook"]},{"location":"Vision/DeIT/#augmentation","title":"Augmentation","text":"<p>There is a need for heavy augmentation - Rand-Augment - Mixup - CutMix Not applied but interesting - Auto augmentation</p>","tags":["Vision-Transformers","Knowledge-Distillation","facebook"]},{"location":"Vision/DeIT/#distillation-token","title":"Distillation-Token","text":"<p>A different technique compared to the usual Distilation - Use of a teacher-student architecture      - Convnet is a better teacher than Transformers     - Prob because of inductive bias    - Use of a separate distillation token,      - starts out different from the class token, (does different things)     - at the end has high similarity with class token     - It is different from class token - Less training required for good results.</p>","tags":["Vision-Transformers","Knowledge-Distillation","facebook"]},{"location":"Vision/DeIT/#how-trained","title":"How Trained ?","text":"<ul> <li>for soft, use of KL divergence between teacher and student as loss</li> <li>for hard, same as classification loss (we can use label smoothing for soft labels)</li> </ul>","tags":["Vision-Transformers","Knowledge-Distillation","facebook"]},{"location":"Vision/DeIT/#at-inference","title":"At inference","text":"<ul> <li>Use of both class and distillation token </li> </ul>","tags":["Vision-Transformers","Knowledge-Distillation","facebook"]},{"location":"Vision/Image%20Color%20Modes/","title":"Image Color Modes","text":"<p>Yanked from here: https://stackoverflow.com/questions/52307290/what-is-the-difference-between-images-in-p-and-l-mode-in-pil</p> <ul> <li>Normally, images are RGB, which means they have 3 channels, one for red, one for green and one for blue. That normally means that each pixel takes 3 bytes of storage, one for red, one for green and one for blue.</li> <li>If you have a\u00a0<code>P</code>\u00a0mode image, that means it is palettised. That means there is a palette with up to 256 different colours in it, and instead of storing 3 bytes for R, G and B for each pixel, you store 1 byte which is the index into the palette. This confers both advantages and disadvantages. The advantage is that your image requires 1/3 of the space in memory and on disk. The disadvantage is that it can only represent 256 unique colours - so you may get\u00a0banding\u00a0or artefacts.</li> <li>If you have an\u00a0<code>L</code>\u00a0mode image, that means it is a single channel image - normally interpreted as greyscale. The\u00a0<code>L</code>\u00a0means that is just stores the Luminance. It is very compact, but only stores a greyscale, not colour.</li> </ul> <p>View to mode using  <code>image.mode</code></p>"},{"location":"Vision/Noisy%20Student/","title":"Key idea","text":"<ul> <li>Some mash of semi supervised learning, transfer learning</li> <li>We have a <ul> <li>Small dataset: ImageNet with labels</li> <li>Much larger dataset: Internet unlabeled</li> <li>We wish to somehow use the much larger dataset.</li> </ul> </li> <li>The unique approach is that instead of training on the unlabeled dataset for transfer learning, we use the labeled dataset and a teacher model Teacher: Train a model on ImageNet and use it to label a large amount of unlabeled data Student: Train on the new labeled data</li> <li>One would think Student shouldn't be able to outperform the teacher, but in this, the student outperforms the teacher</li> </ul>"},{"location":"Vision/Noisy%20Student/#key-addition","title":"Key Addition","text":"<ul> <li>Add a lot of noise(augmentation) on the big dataset (added robustness) and a lot of noise on the model(dropout, stochastic depth)</li> <li>Student is larger than the teacher  </li> </ul>"},{"location":"Vision/Noisy%20Student/#some-details","title":"Some Details","text":"<ul> <li>Teacher labels can be hard or soft pseudo labels. (Soft work better)</li> <li>Larger Teacher -&gt; better</li> <li>A ton of data required</li> <li>Increase batch size of the student</li> <li>Teacher: gives pseudo labels to un-augmented images, Student: trained on augmented images</li> <li>Filter images the teacher model has low confidence on (probably they are out of domain images)</li> <li>Balance the unlabeled images class wise, so that they match the original distribution. (dublication: upsampling /talking images with higher confidence: downsampling)</li> <li>Iterative training</li> <li>Joint Training on labeled and unlabeled data.</li> <li>Training student from scratch maybe better</li> <li>All this introduces robustness to pertubations on the input images, better ability to tackle adversarial input</li> </ul>"},{"location":"Vision/SWIN/","title":"SWIN","text":"<p>A Vision Transformer with a hierarchical structure. - Starts with smaller patches \\((4\\times4\\times3)\\) - Patches are merged to form bigger patches Linearly scaling Shifted Window attention mechanism For tasks where you need a more granular input to transformer. Goal of using Transformer-based models as a general purpose vision backbone.</p>","tags":["Vision-Transformers"]},{"location":"Vision/SWIN/#why","title":"Why ?","text":"<ul> <li>The fixed transformer token size is not suitable for images where elements can vary in sizes</li> <li>For tasks like segmentation, we need more dense prediction</li> <li>Attention mechanism has a quadratic cost.</li> </ul>","tags":["Vision-Transformers"]},{"location":"Vision/SWIN/#architectural-overview","title":"Architectural Overview","text":"<p> - Start with patches of \\(4 \\times 4 \\times 3 = 48\\), which is then linearly transformed to a size of \\(C\\). Basically \\(d_{model}\\). - Standard transformer block replaced by SWIN Transformer block, activation is GELU.</p>","tags":["Vision-Transformers"]},{"location":"Vision/SWIN/#positional-embedding","title":"Positional Embedding","text":"<ul> <li>Relative positioning works better.</li> </ul>","tags":["Vision-Transformers"]},{"location":"Vision/SWIN/#attention-mechanism","title":"Attention Mechanism","text":"<p>Linearly scaling attention mechanism is achieved by limiting the attention to a non overlapping window. We Get a window that contains \\(M \\times M\\) patches. The window approach limits the attention to only a local region, and to overcome this limitation, they introduce Shifted Window</p>","tags":["Vision-Transformers"]},{"location":"Vision/SWIN/#shifted-window","title":"Shifted-Window","text":"<p>The widow for self attention is shifted in consecutive self attention layers. Attention span is limited to a non overlapping window. The window is then shifted across the main diagonal, downwards, allowing for cross-window attention.  - For efficient computation, use of cyclic-shifting towards top-left direction. </p>","tags":["Vision-Transformers"]},{"location":"Vision/SWIN/#relative-position-bias","title":"Relative Position Bias","text":"<p>A relative positional bias added at self attention layer. \\(\\(Attention(Q, K, V ) = SoftMax(QKT / \u221a d + B)V\\)\\) Read more on Transformer Embeddings#Relative-Positioning</p>","tags":["Vision-Transformers"]},{"location":"Vision/SWIN/#merging-layer","title":"Merging-Layer","text":"<ul> <li>Adjacent 4 patches are merged into one patch, in the image space.</li> <li>Information of 4 patches is merged into one, and this new patch has double the hidden representation. \\(\\(\\large 4 \\times C \\xrightarrow[\\text{}]{\\text{merge}} 4C \\xrightarrow{linear} 2C\\)\\) https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper</li> </ul>","tags":["Vision-Transformers"]},{"location":"Vision/Segmentation/","title":"Segmentation","text":"<pre><code># ablumentations for easy image augmentation for input as well as output\nimport albumentations`\n</code></pre>"},{"location":"Vision/Segmentation/#3d-segmentation","title":"3D Segmentation","text":"<p>Approaches - 2D slices: This technique not too good as a lot of information gets lost. You need information about the lower and upper slices to make sense of current slice. -  - 3D (CT-scans; nnUNet)  - 2D (normal images; US; X-rays) </p>"},{"location":"Vision/ViT/","title":"ViT","text":"<p>Vision Transformers</p>","tags":["Vision-Transformers","Google"]},{"location":"Vision/ViT/#overview","title":"Overview","text":"<ul> <li>Originally introduced at https://arxiv.org/abs/2010.11929</li> <li>Only Encoder used, proposes a pure transformer model for sequence of image patches.<ul> <li>Pass through a linear embedding layer</li> <li>Tokens for transformer</li> </ul> </li> <li>Pre-trained on image data<ul> <li>ImageNet, ... for classifcation task and then the head removed</li> <li>require less computation</li> </ul> </li> <li>Why Transformers?<ul> <li>Because Transformers</li> <li>Also because they lack ViT#Inductive Bias, present in CNNs</li> <li>And if we train on large enough dataset, we do not need any ViT#Inductive Bias </li> </ul> </li> </ul>","tags":["Vision-Transformers","Google"]},{"location":"Vision/ViT/#related-works","title":"Related works","text":"<ul> <li>BERT trained using self-supervised pre-training task, [[GPT]] with language modeling</li> <li>For Images<ul> <li>Pixel-wise is too expensive</li> <li>Self attention to only local areas: can completely replace transformers.</li> <li>Scalable approximation to global self attention a paper</li> <li>2x2 patches and self attention (works for small images)</li> </ul> </li> </ul>","tags":["Vision-Transformers","Google"]},{"location":"Vision/ViT/#inductive-bias","title":"Inductive Bias","text":"<p>Theoretically a Fully connected neural network can approximate any function and thus any mapping. We design certain network structures, using our understanding of a domain, so that a network can more easily learn about that particular domain This introduces a bias, that helps the network do well. - CNN:      - Locality     - Translation Equivalent      - 2 Dimensional structure - RNN:     - The Sequence nature of the data These are imposed from human understanding of the domain(images &amp; language). We call them biases as they in a way restrict model's ability to learn a more general function. ? - Good for less amount of data</p>","tags":["Vision-Transformers","Google"]},{"location":"Vision/ViT/#no-need-for-inductive-biases","title":"No need for inductive biases","text":"<ul> <li>Inductive Bias helps when data is less. </li> <li>But then data is plenty(303M or more idk), they can hinder </li> </ul>","tags":["Vision-Transformers","Google"]},{"location":"Vision/ViT/#details","title":"Details","text":"","tags":["Vision-Transformers","Google"]},{"location":"Vision/ViT/#steps","title":"Steps","text":"","tags":["Vision-Transformers","Google"]},{"location":"Vision/ViT/#embedding","title":"Embedding","text":"<ul> <li>Data -&gt; Vectors</li> <li>In NLP, the data is generally a token</li> <li>In VIT: <ul> <li>transform image into 16 x 16 patches</li> <li>Rearrange('b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)')</li> <li>Fully connected neurons \\(\\(\\Large x \\in R^{H \\times W \\times C} \\rightarrow x_p \\in R^{N \\times (P^2C)} \\Rightarrow z_0 \\in R^{(1+N) \\times D}\\)\\) \\(\\rightarrow\\): Reshaping \\(\\Rightarrow\\): Trainable Linear projection We add an extra token as the {class} token</li> </ul> </li> <li>Use of 1 D Learnable Positional Embedding, on later analysis, they have some aspect of the  2 D shape</li> </ul>","tags":["Vision-Transformers","Google"]},{"location":"Vision/ViT/#positional-embedding","title":"Positional Embedding","text":"<ul> <li>1 D positional embedding.</li> <li>Trained on lower Image size, fine tuned on higher</li> <li>Interpolation done for positional embeddings</li> </ul>","tags":["Vision-Transformers","Google"]},{"location":"Vision/ViT/#pre-training","title":"Pre-Training","text":"<ul> <li>Use of MLP with one hidden layer, GLUE non linearity</li> <li>Use of \\(z_l^0\\)</li> </ul>","tags":["Vision-Transformers","Google"]},{"location":"Vision/ViT/#fine-tuning","title":"Fine Tuning","text":"<ul> <li>Attach a zero-initialized \\(D \\times K\\) feed-forward layer</li> <li>When images are of higher resolution<ul> <li>keep the patch size</li> <li>sequence length becomes larger and positional embedding lose meaning</li> <li>2D interpolation of pre-trained positional embeddings</li> </ul> </li> </ul>","tags":["Vision-Transformers","Google"]},{"location":"Vision/ViT/#experimental-setups","title":"Experimental Setups","text":"<ul> <li>Hybrid: Feeding intermediate features extracted from a CNN</li> <li>Evaluated using few shots (when there are many examples to fine tune) and fine tune accuracy</li> </ul>","tags":["Vision-Transformers","Google"]},{"location":"Vision/ViT/#observations-from-vit-layers","title":"Observations from ViT layers","text":"<ul> <li>CNN Filter like structures learned at the initial linear embedding layer</li> <li>Closer patches have similar positional embeddings<ul> <li>Form a 2D topology: the embeddings store information about their position in image</li> </ul> </li> <li>Ability to attend to global information is used by the transformers from the start</li> </ul>","tags":["Vision-Transformers","Google"]},{"location":"Vision/ViT/#conclusions","title":"Conclusions","text":"<ul> <li>To use ViT effectively, it requires a lot of data, as only then, will we not require the inductive bias.</li> <li>If not enough data is available CNN structures perform better</li> <li>ViT better performance/compute tradeoff, hybrids better at lower computational capabilities</li> </ul> <p>https://www.youtube.com/watch?v=j3VNqtJUoz0&amp;t=329s https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/papers/vit.pdf https://openaccess.thecvf.com/content/ICCV2021/html/Yuan_Tokens-to-Token_ViT_Training_Vision_Transformers_From_Scratch_on_ImageNet_ICCV_2021_paper.html?ref=https://githubhelp.com https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper</p>","tags":["Vision-Transformers","Google"]},{"location":"Vision/nnU-Net/","title":"nnU Net","text":"<ul> <li>U-net allowed for ability to customize exact architecture</li> <li>nnU-Net: A more focused approach to U-net, focus on aspects that make out the perfor- mance and generalizability of a method</li> <li>Robust and Self adapting framework??</li> </ul>"},{"location":"Vision/nnU-Net/#problem","title":"Problem","text":"<ul> <li>For sota performance, requirement of modification of arch of unet</li> <li>arch validation on (a lot of times) a single dataset.</li> <li>hard to validate superiority of arch, for the general case (across multiple datasets).</li> </ul>"},{"location":"Vision/nnU-Net/#-arch-tweeks-can-be-shown-to-imporve-performance-for-an-unoptimized-networks-but-according-to-the-paper-are-unable-to-make-results-better-for-fully-optimized-one-and-push-sota","title":"- arch tweeks can be shown to imporve performance for an unoptimized networks, but according to the paper, are unable to make results better for fully optimized one and push sota.","text":""},{"location":"Vision/nnU-Net/#medical-segmentation-decathlon","title":"Medical Segmentation Decathlon","text":"<ul> <li>participants asked to create a segmentation algorithm that generalizes across 10 datasets corresponding to different entities of the human body</li> <li>allowed to adapt to the dataset but must do so in an automated manner. 1) a development phase in which participants are given access to 7 datasets to optimize their approach on and, using their final and thus frozen method, must submit segmentations for the corresponding 7 held-out test sets.  2) a second phase to evaluate the same exact method on 3 previously undisclosed datasets.</li> </ul>"},{"location":"blog/","title":"Blog","text":""}]}